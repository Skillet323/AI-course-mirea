{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1259d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports & Settings\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    davies_bouldin_score,\n",
    "    calinski_harabasz_score,\n",
    ")\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path(\".\")\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "ARTIFACTS_DIR = BASE_DIR / \"artifacts\"\n",
    "FIGS_DIR = ARTIFACTS_DIR / \"figures\"\n",
    "LABELS_DIR = ARTIFACTS_DIR / \"labels\"\n",
    "for p in (ARTIFACTS_DIR, FIGS_DIR, LABELS_DIR):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DEFAULT_DATA_FILES = [\n",
    "    DATA_DIR / \"S07-hw-dataset-02.csv\",\n",
    "    DATA_DIR / \"S07-hw-dataset-03.csv\",\n",
    "    DATA_DIR / \"S07-hw-dataset-04.csv\",\n",
    "]\n",
    "\n",
    "DATA_FILES = DEFAULT_DATA_FILES\n",
    "\n",
    "# Helper printing\n",
    "def info(msg: str):\n",
    "    print(f\"[INFO] {msg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20d87e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Utility functions\n",
    "def safe_read_csv(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Dataset file not found: {path}\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def build_preprocessor(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Build ColumnTransformer:\n",
    "    - Numeric: SimpleImputer(mean) -> StandardScaler\n",
    "    - Categorical: SimpleImputer(constant) -> OneHotEncoder(handle_unknown='ignore')\n",
    "    Returns transformer and list of numeric/categorical column names.\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    # exclude sample_id if present\n",
    "    numeric_cols = [c for c in numeric_cols if c != \"sample_id\"]\n",
    "    cat_cols = [c for c in df.columns if c not in numeric_cols and c != \"sample_id\"]\n",
    "    # numeric pipeline\n",
    "    numeric_transformer = (\n",
    "        (\"num\", SimpleImputer(strategy=\"mean\"), numeric_cols)\n",
    "        if numeric_cols\n",
    "        else None\n",
    "    )\n",
    "    # categorical pipeline\n",
    "    categorical_transformer = (\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False), cat_cols)\n",
    "        if cat_cols\n",
    "        else None\n",
    "    )\n",
    "    transformers = []\n",
    "    if numeric_transformer:\n",
    "        transformers.append(numeric_transformer)\n",
    "    if categorical_transformer:\n",
    "        transformers.append(categorical_transformer)\n",
    "    # Build ColumnTransformer that imputes and encodes; scaling applied after transform\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "\n",
    "    ct = ColumnTransformer(transformers, remainder=\"drop\", sparse_threshold=0)\n",
    "    return ct, numeric_cols, cat_cols\n",
    "\n",
    "def preprocess_fit_transform(ct: ColumnTransformer, df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Fit ColumnTransformer on df and return transformed numpy array and feature names.\n",
    "    Then apply StandardScaler to numeric+encoded features.\n",
    "    \"\"\"\n",
    "    X_pre = ct.fit_transform(df)\n",
    "    # Build feature names for transformed output (best-effort)\n",
    "    feat_names = []\n",
    "    for name, transformer, cols in ct.transformers_:\n",
    "        if transformer is None:\n",
    "            continue\n",
    "        if hasattr(transformer, \"get_feature_names_out\"):\n",
    "            try:\n",
    "                out_names = transformer.get_feature_names_out(cols)\n",
    "            except Exception:\n",
    "                out_names = cols\n",
    "        else:\n",
    "            out_names = cols\n",
    "        feat_names.extend([str(n) for n in out_names])\n",
    "    # scale\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_pre)\n",
    "    return X_scaled, feat_names, ct, scaler\n",
    "\n",
    "def preprocess_transform(ct: ColumnTransformer, scaler: StandardScaler, df: pd.DataFrame):\n",
    "    X_pre = ct.transform(df)\n",
    "    X_scaled = scaler.transform(X_pre)\n",
    "    return X_scaled\n",
    "\n",
    "def compute_internal_metrics(X: np.ndarray, labels: np.ndarray, consider_noise_as_cluster=False):\n",
    "    \"\"\"\n",
    "    Compute silhouette, davies_bouldin, calinski_harabasz.\n",
    "    For labels where one cluster or all noise, return None appropriately.\n",
    "    \"\"\"\n",
    "    metrics = {\"silhouette\": None, \"davies_bouldin\": None, \"calinski_harabasz\": None}\n",
    "    # number of valid clusters\n",
    "    unique_labels = np.unique(labels)\n",
    "    # If DBSCAN noise present and consider_noise_as_cluster False, we compute metrics on non-noise only\n",
    "    if -1 in unique_labels and (not consider_noise_as_cluster):\n",
    "        non_noise_mask = labels != -1\n",
    "        if non_noise_mask.sum() < 2:\n",
    "            return metrics, {\"n_points_non_noise\": int(non_noise_mask.sum()), \"n_clusters_non_noise\": 0}\n",
    "        X_eval = X[non_noise_mask]\n",
    "        labels_eval = labels[non_noise_mask]\n",
    "    else:\n",
    "        X_eval = X\n",
    "        labels_eval = labels\n",
    "    n_clusters = len(set(labels_eval))\n",
    "    if n_clusters <= 1 or X_eval.shape[0] <= 1:\n",
    "        return metrics, {\"n_points_non_noise\": int((labels != -1).sum()), \"n_clusters_non_noise\": n_clusters}\n",
    "    try:\n",
    "        metrics[\"silhouette\"] = float(silhouette_score(X_eval, labels_eval))\n",
    "    except Exception:\n",
    "        metrics[\"silhouette\"] = None\n",
    "    try:\n",
    "        metrics[\"davies_bouldin\"] = float(davies_bouldin_score(X_eval, labels_eval))\n",
    "    except Exception:\n",
    "        metrics[\"davies_bouldin\"] = None\n",
    "    try:\n",
    "        metrics[\"calinski_harabasz\"] = float(calinski_harabasz_score(X_eval, labels_eval))\n",
    "    except Exception:\n",
    "        metrics[\"calinski_harabasz\"] = None\n",
    "    return metrics, {\"n_points_non_noise\": int((labels != -1).sum()), \"n_clusters_non_noise\": n_clusters}\n",
    "\n",
    "def pca_2d(X: np.ndarray, random_state=RANDOM_STATE):\n",
    "    pca = PCA(n_components=2, random_state=random_state)\n",
    "    X2 = pca.fit_transform(X)\n",
    "    return X2, pca\n",
    "\n",
    "def save_fig(fig, path: Path):\n",
    "    fig.savefig(path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0af3f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing dataset 1: homeworks\\HW07\\data\\S07-hw-dataset-02.csv\n",
      "[INFO] File homeworks\\HW07\\data\\S07-hw-dataset-02.csv not found — skipping\n",
      "[INFO] Processing dataset 2: homeworks\\HW07\\data\\S07-hw-dataset-03.csv\n",
      "[INFO] File homeworks\\HW07\\data\\S07-hw-dataset-03.csv not found — skipping\n",
      "[INFO] Processing dataset 3: homeworks\\HW07\\data\\S07-hw-dataset-04.csv\n",
      "[INFO] File homeworks\\HW07\\data\\S07-hw-dataset-04.csv not found — skipping\n"
     ]
    }
   ],
   "source": [
    "# 3. Main experiment per dataset\n",
    "metrics_summary = {}\n",
    "best_configs = {}\n",
    "\n",
    "processed_datasets = []  # store dataset names processed\n",
    "\n",
    "for idx, data_file in enumerate(DATA_FILES, start=1):\n",
    "    dataset_name = data_file.stem\n",
    "    info(f\"Processing dataset {idx}: {data_file}\")\n",
    "    if not data_file.exists():\n",
    "        info(f\"File {data_file} not found — skipping\")\n",
    "        continue\n",
    "    df = safe_read_csv(data_file)\n",
    "    processed_datasets.append(dataset_name)\n",
    "\n",
    "    info(\"Basic EDA:\")\n",
    "    info(f\" - shape: {df.shape}\")\n",
    "    info(f\" - head:\\n{df.head().to_string(index=False)}\")\n",
    "    info(f\" - dtypes:\\n{df.dtypes}\")\n",
    "    # missing\n",
    "    miss = df.isna().sum()\n",
    "    info(f\" - missing summary (top):\\n{miss[miss>0].sort_values(ascending=False).head().to_string() or 'No missing values'}\")\n",
    "\n",
    "    # Separate sample_id if exists\n",
    "    sample_ids = df[\"sample_id\"] if \"sample_id\" in df.columns else pd.Series(np.arange(len(df)), name=\"sample_id\")\n",
    "    X_df = df.drop(columns=[\"sample_id\"]) if \"sample_id\" in df.columns else df.copy()\n",
    "\n",
    "    info(\"Building preprocessor (imputation, encoding)...\")\n",
    "    ct, numeric_cols, cat_cols = build_preprocessor(X_df)\n",
    "    try:\n",
    "        X_scaled, feat_names, ct_fitted, scaler_fitted = preprocess_fit_transform(ct, X_df)\n",
    "        info(f\" - transformed shape: {X_scaled.shape}\")\n",
    "    except Exception as exc:\n",
    "        info(f\"Preprocessing failed: {exc}\")\n",
    "        continue\n",
    "\n",
    "    info(\"Searching K for KMeans (2..12)\")\n",
    "    k_range = list(range(2, min(13, max(3, X_scaled.shape[0]//5))))  # adaptive upper bound\n",
    "    k_range = k_range if k_range else [2,3,4]\n",
    "    k_metrics = []\n",
    "    for k in k_range:\n",
    "        try:\n",
    "            km = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=10)\n",
    "            labels_k = km.fit_predict(X_scaled)\n",
    "            met, extra = compute_internal_metrics(X_scaled, labels_k, consider_noise_as_cluster=True)\n",
    "            k_metrics.append({\"k\": k, **met, \"n_clusters\": len(set(labels_k))})\n",
    "        except Exception as exc:\n",
    "            k_metrics.append({\"k\": k, \"silhouette\": None, \"davies_bouldin\": None, \"calinski_harabasz\": None, \"n_clusters\": None})\n",
    "    kdf = pd.DataFrame(k_metrics)\n",
    "\n",
    "    # Plot silhouette vs k\n",
    "    try:\n",
    "        fig, ax = plt.subplots(figsize=(6,4))\n",
    "        ax.plot(kdf[\"k\"], kdf[\"silhouette\"], marker=\"o\")\n",
    "        ax.set_xlabel(\"k (KMeans)\")\n",
    "        ax.set_ylabel(\"Silhouette score\")\n",
    "        ax.set_title(f\"{dataset_name}: Silhouette vs k (KMeans)\")\n",
    "        path = FIGS_DIR / f\"{dataset_name}_silhouette_vs_k.png\"\n",
    "        save_fig(fig, path)\n",
    "        info(f\"Saved figure {path}\")\n",
    "    except Exception as exc:\n",
    "        info(f\"Failed to plot silhouette vs k: {exc}\")\n",
    "\n",
    "    # Choose best k by silhouette (primary), break ties by calinski_harabasz\n",
    "    kdf_valid = kdf.dropna(subset=[\"silhouette\"])\n",
    "    if not kdf_valid.empty:\n",
    "        best_k_row = kdf_valid.sort_values([\"silhouette\", \"calinski_harabasz\"], ascending=[False, False]).iloc[0]\n",
    "        best_k = int(best_k_row[\"k\"])\n",
    "    else:\n",
    "        best_k = int(k_range[0])\n",
    "    info(f\"Selected KMeans k = {best_k}\")\n",
    "\n",
    "    best_kmeans = KMeans(n_clusters=best_k, random_state=RANDOM_STATE, n_init=10)\n",
    "    labels_kmeans = best_kmeans.fit_predict(X_scaled)\n",
    "    metrics_kmeans, extra_kmeans = compute_internal_metrics(X_scaled, labels_kmeans, consider_noise_as_cluster=True)\n",
    "\n",
    "    # We'll attempt DBSCAN with eps grid; if DBSCAN produces too much noise/unusable, we'll fallback to Agglomerative\n",
    "    info(\"Trying DBSCAN parameter sweep (eps grid)\")\n",
    "    # Use heuristic eps range based on pairwise distances median (approx) - use kNN distances would be better but keep simple\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "    try:\n",
    "        # estimate a scale: distance to 5th neighbor median\n",
    "        neigh = NearestNeighbors(n_neighbors=min(10, max(2, X_scaled.shape[0]//10))).fit(X_scaled)\n",
    "        dists, _ = neigh.kneighbors(X_scaled)\n",
    "        # use distance to 4th or median of 5th neighbor if available\n",
    "        kth = min(4, dists.shape[1]-1)\n",
    "        kth_dists = dists[:, kth]\n",
    "        median_kth = float(np.median(kth_dists))\n",
    "        eps_vals = np.unique(np.concatenate([np.linspace(median_kth*0.3, median_kth*1.5, 8), np.linspace(median_kth*0.1, median_kth*3.0, 6)]))\n",
    "    except Exception:\n",
    "        eps_vals = np.linspace(0.1, 2.0, 8)\n",
    "\n",
    "    dbscan_metrics = []\n",
    "    for eps in eps_vals:\n",
    "        try:\n",
    "            db = DBSCAN(eps=float(eps), min_samples=5)\n",
    "            labels_db = db.fit_predict(X_scaled)\n",
    "            # compute noise fraction\n",
    "            noise_frac = float((labels_db == -1).sum()) / len(labels_db)\n",
    "            # compute metrics on non-noise points if at least 2 clusters\n",
    "            met, extra = compute_internal_metrics(X_scaled, labels_db, consider_noise_as_cluster=False)\n",
    "            dbscan_metrics.append({\"eps\": float(eps), \"noise_frac\": noise_frac, **met, \"n_clusters_non_noise\": extra[\"n_clusters_non_noise\"]})\n",
    "        except Exception:\n",
    "            dbscan_metrics.append({\"eps\": float(eps), \"noise_frac\": None, \"silhouette\": None, \"davies_bouldin\": None, \"calinski_harabasz\": None, \"n_clusters_non_noise\": None})\n",
    "    dbdf = pd.DataFrame(dbscan_metrics)\n",
    "\n",
    "    # Plot silhouette vs eps (for DBSCAN non-noise)\n",
    "    try:\n",
    "        fig, ax = plt.subplots(figsize=(6,4))\n",
    "        ax.plot(dbdf[\"eps\"], dbdf[\"silhouette\"], marker=\"o\")\n",
    "        ax.set_xlabel(\"eps (DBSCAN)\")\n",
    "        ax.set_ylabel(\"Silhouette (non-noise)\")\n",
    "        ax.set_title(f\"{dataset_name}: DBSCAN silhouette vs eps\")\n",
    "        path = FIGS_DIR / f\"{dataset_name}_dbscan_silhouette_vs_eps.png\"\n",
    "        save_fig(fig, path)\n",
    "        info(f\"Saved figure {path}\")\n",
    "    except Exception as exc:\n",
    "        info(f\"Failed to plot DBSCAN silhouette vs eps: {exc}\")\n",
    "\n",
    "    # Choose best DBSCAN by silhouette for non-noise points, but also prefer low noise fraction (<0.5)\n",
    "    dbdf_valid = dbdf.dropna(subset=[\"silhouette\"])\n",
    "    dbdf_valid_filtered = dbdf_valid[dbdf_valid[\"noise_frac\"] <= 0.5] if not dbdf_valid.empty else dbdf_valid\n",
    "    if not dbdf_valid_filtered.empty:\n",
    "        best_db_row = dbdf_valid_filtered.sort_values([\"silhouette\", \"calinski_harabasz\"], ascending=[False, False]).iloc[0]\n",
    "        best_eps = float(best_db_row[\"eps\"])\n",
    "    elif not dbdf_valid.empty:\n",
    "        best_db_row = dbdf_valid.sort_values([\"silhouette\", \"calinski_harabasz\"], ascending=[False, False]).iloc[0]\n",
    "        best_eps = float(best_db_row[\"eps\"])\n",
    "    else:\n",
    "        best_eps = None\n",
    "\n",
    "    if best_eps is not None:\n",
    "        info(f\"Selected DBSCAN eps = {best_eps}\")\n",
    "        best_db = DBSCAN(eps=best_eps, min_samples=5)\n",
    "        labels_db_final = best_db.fit_predict(X_scaled)\n",
    "        metrics_db, extra_db = compute_internal_metrics(X_scaled, labels_db_final, consider_noise_as_cluster=False)\n",
    "    else:\n",
    "        info(\"DBSCAN did not produce valid clustering; falling back to AgglomerativeClustering\")\n",
    "        # Agglomerative: try linkage variants and a few k values\n",
    "        aggl_metrics = []\n",
    "        for linkage in [\"ward\", \"complete\", \"average\"]:\n",
    "            # ward requires euclidean and no categorical encoding issues (we have numeric scaled features)\n",
    "            for k in range(2, min(8, X_scaled.shape[0]//5 + 3)):\n",
    "                try:\n",
    "                    agg = AgglomerativeClustering(n_clusters=k, linkage=linkage)\n",
    "                    labels_agg = agg.fit_predict(X_scaled)\n",
    "                    met, extra = compute_internal_metrics(X_scaled, labels_agg, consider_noise_as_cluster=True)\n",
    "                    aggl_metrics.append({\"linkage\": linkage, \"k\": k, **met})\n",
    "                except Exception:\n",
    "                    aggl_metrics.append({\"linkage\": linkage, \"k\": k, \"silhouette\": None, \"davies_bouldin\": None, \"calinski_harabasz\": None})\n",
    "        aggl_df = pd.DataFrame(aggl_metrics)\n",
    "        aggl_valid = aggl_df.dropna(subset=[\"silhouette\"])\n",
    "        if not aggl_valid.empty:\n",
    "            best_aggl = aggl_valid.sort_values([\"silhouette\", \"calinski_harabasz\"], ascending=[False, False]).iloc[0]\n",
    "            best_linkage = best_aggl[\"linkage\"]\n",
    "            best_k_aggl = int(best_aggl[\"k\"])\n",
    "        else:\n",
    "            best_linkage = \"ward\"\n",
    "            best_k_aggl = 2\n",
    "        info(f\"Selected Agglomerative linkage={best_linkage}, k={best_k_aggl}\")\n",
    "        best_agg_model = AgglomerativeClustering(n_clusters=best_k_aggl, linkage=best_linkage)\n",
    "        labels_db_final = best_agg_model.fit_predict(X_scaled)\n",
    "        metrics_db, extra_db = compute_internal_metrics(X_scaled, labels_db_final, consider_noise_as_cluster=True)\n",
    "\n",
    "    # We pick best among KMeans and (DBSCAN or Aggl) by primary metric silhouette (non-noise for DBSCAN)\n",
    "    # Prepare metric records\n",
    "    record = {}\n",
    "    record[\"kmeans\"] = {\"params\": {\"k\": best_k}, \"metrics\": metrics_kmeans, \"n_clusters\": int(len(set(labels_kmeans)))}\n",
    "    record[\"second\"] = {}\n",
    "    if best_eps is not None:\n",
    "        record[\"second\"][\"algo\"] = \"DBSCAN\"\n",
    "        record[\"second\"][\"params\"] = {\"eps\": best_eps, \"min_samples\": 5}\n",
    "        record[\"second\"][\"metrics\"] = metrics_db\n",
    "        record[\"second\"][\"noise_frac\"] = float((labels_db_final == -1).sum()) / len(labels_db_final)\n",
    "        n_clusters_second = int(len(set(labels_db_final)) - (1 if -1 in labels_db_final else 0))\n",
    "    else:\n",
    "        record[\"second\"][\"algo\"] = \"Agglomerative\"\n",
    "        record[\"second\"][\"params\"] = {\"linkage\": best_linkage, \"k\": best_k_aggl}\n",
    "        record[\"second\"][\"metrics\"] = metrics_db\n",
    "        record[\"second\"][\"noise_frac\"] = None\n",
    "        n_clusters_second = int(len(set(labels_db_final)))\n",
    "\n",
    "    # Compare silhouette (note: DBSCAN metrics computed on non-noise) -> choose best\n",
    "    s_k = record[\"kmeans\"][\"metrics\"].get(\"silhouette\")\n",
    "    s_s = record[\"second\"][\"metrics\"].get(\"silhouette\")\n",
    "    # For cases where silhouette None, use calinski_harabasz as fallback (higher better)\n",
    "    def pick_best(s1, s2, m1, m2):\n",
    "        if s1 is not None and s2 is not None:\n",
    "            return \"kmeans\" if s1 >= s2 else \"second\"\n",
    "        if s1 is not None:\n",
    "            return \"kmeans\"\n",
    "        if s2 is not None:\n",
    "            return \"second\"\n",
    "        # fallback to ch\n",
    "        ch1 = m1.get(\"calinski_harabasz\")\n",
    "        ch2 = m2.get(\"calinski_harabasz\")\n",
    "        if ch1 is not None and ch2 is not None:\n",
    "            return \"kmeans\" if ch1 >= ch2 else \"second\"\n",
    "        return \"kmeans\"\n",
    "\n",
    "    best_choice_key = pick_best(\n",
    "        s_k, s_s, record[\"kmeans\"][\"metrics\"], record[\"second\"][\"metrics\"]\n",
    "    )\n",
    "    if best_choice_key == \"kmeans\":\n",
    "        chosen_algo = \"KMeans\"\n",
    "        chosen_labels = labels_kmeans\n",
    "        chosen_params = record[\"kmeans\"][\"params\"]\n",
    "        chosen_metrics = record[\"kmeans\"][\"metrics\"]\n",
    "    else:\n",
    "        chosen_algo = record[\"second\"][\"algo\"]\n",
    "        chosen_labels = labels_db_final\n",
    "        chosen_params = record[\"second\"][\"params\"]\n",
    "        chosen_metrics = record[\"second\"][\"metrics\"]\n",
    "\n",
    "    info(f\"Chosen best for {dataset_name}: {chosen_algo}, params={chosen_params}, metrics={chosen_metrics}\")\n",
    "\n",
    "    try:\n",
    "        X2, pca_model = pca_2d(X_scaled)\n",
    "        # scatter colored by cluster\n",
    "        unique_lbls = np.unique(chosen_labels)\n",
    "        palette = plt.get_cmap(\"tab20\")\n",
    "        fig, ax = plt.subplots(figsize=(6,5))\n",
    "        for i, lbl in enumerate(unique_lbls):\n",
    "            mask = chosen_labels == lbl\n",
    "            ax.scatter(\n",
    "                X2[mask, 0],\n",
    "                X2[mask, 1],\n",
    "                s=20,\n",
    "                alpha=0.8,\n",
    "                label=f\"cluster {int(lbl)}\" if lbl != -1 else \"noise (-1)\",\n",
    "                color=palette(i % 20),\n",
    "                edgecolors=\"k\" if chosen_algo != \"DBSCAN\" else None,\n",
    "                linewidths=0.2,\n",
    "            )\n",
    "        ax.set_title(f\"{dataset_name} PCA(2D) - {chosen_algo}\")\n",
    "        ax.legend(loc=\"best\", fontsize=\"small\", markerscale=2)\n",
    "        ax.set_xlabel(\"PC1\")\n",
    "        ax.set_ylabel(\"PC2\")\n",
    "        path = FIGS_DIR / f\"{dataset_name}_pca2d_{chosen_algo}.png\"\n",
    "        save_fig(fig, path)\n",
    "        info(f\"Saved PCA scatter to {path}\")\n",
    "    except Exception as exc:\n",
    "        info(f\"Failed PCA visualization: {exc}\")\n",
    "\n",
    "    labels_out = pd.DataFrame({\"sample_id\": sample_ids.values, \"cluster_label\": chosen_labels})\n",
    "    labels_csv_path = LABELS_DIR / f\"labels_{dataset_name}.csv\"\n",
    "    labels_out.to_csv(labels_csv_path, index=False)\n",
    "    info(f\"Saved labels to {labels_csv_path}\")\n",
    "\n",
    "    metrics_summary[dataset_name] = {\n",
    "        \"kmeans\": record[\"kmeans\"],\n",
    "        \"second\": record[\"second\"],\n",
    "        \"chosen\": {\"algo\": chosen_algo, \"params\": chosen_params, \"metrics\": chosen_metrics},\n",
    "    }\n",
    "    best_configs[dataset_name] = {\"algo\": chosen_algo, \"params\": chosen_params, \"selection_reason\": \"silhouette/CH heuristics\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf1f9478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Writing summary artifacts (metrics_summary.json, best_configs.json)...\n",
      "[INFO] Artifact JSONs written.\n"
     ]
    }
   ],
   "source": [
    "# 4. Save artifacts JSONs\n",
    "info(\"Writing summary artifacts (metrics_summary.json, best_configs.json)...\")\n",
    "with open(ARTIFACTS_DIR / \"metrics_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics_summary, f, ensure_ascii=False, indent=2)\n",
    "with open(ARTIFACTS_DIR / \"best_configs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(best_configs, f, ensure_ascii=False, indent=2)\n",
    "info(\"Artifact JSONs written.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55d75a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Less than 6 figures found — generating additional diagnostic plots to meet artifact requirements.\n",
      "[INFO] Total figures in homeworks\\HW07\\artifacts\\figures: 0\n"
     ]
    }
   ],
   "source": [
    "# 5. Ensure at least 6 figures exist\n",
    "# Count existing figures\n",
    "existing_figs = list(FIGS_DIR.glob(\"*.png\"))\n",
    "if len(existing_figs) < 6:\n",
    "    info(\"Less than 6 figures found — generating additional diagnostic plots to meet artifact requirements.\")\n",
    "    # For each processed dataset, if silhouette vs k or DBSCAN plots exist, copy or regenerate simple histograms\n",
    "    for ds in processed_datasets:\n",
    "        # simple histogram of first two PCA components if available\n",
    "        try:\n",
    "            # load labels file\n",
    "            labf = LABELS_DIR / f\"labels_{ds}.csv\"\n",
    "            if labf.exists():\n",
    "                labdf = pd.read_csv(labf)\n",
    "                # recreate PCA plot if not present\n",
    "                pca_plot = FIGS_DIR / f\"{ds}_pca2d_KMeans.png\"\n",
    "                if not pca_plot.exists():\n",
    "                    # attempt to recreate quick using data\n",
    "                    df = pd.read_csv(DATA_DIR / f\"{ds}.csv\") if (DATA_DIR / f\"{ds}.csv\").exists() else None\n",
    "                    if df is not None:\n",
    "                        # quick preprocessing naive\n",
    "                        if \"sample_id\" in df.columns:\n",
    "                            sid = df[\"sample_id\"]\n",
    "                            Xdf = df.drop(columns=[\"sample_id\"])\n",
    "                        else:\n",
    "                            sid = pd.Series(np.arange(len(df)), name=\"sample_id\")\n",
    "                            Xdf = df.copy()\n",
    "                        # numeric only\n",
    "                        Xnum = Xdf.select_dtypes(include=[np.number]).fillna(0).values\n",
    "                        if Xnum.shape[1] >= 2:\n",
    "                            pca = PCA(n_components=2, random_state=RANDOM_STATE)\n",
    "                            X2 = pca.fit_transform(StandardScaler().fit_transform(Xnum))\n",
    "                            fig, ax = plt.subplots(figsize=(6,4))\n",
    "                            ax.scatter(X2[:,0], X2[:,1], s=10)\n",
    "                            ax.set_title(f\"{ds} quick PCA\")\n",
    "                            save_fig(fig, FIGS_DIR / f\"{ds}_quick_pca.png\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    # refresh existing_figs\n",
    "    existing_figs = list(FIGS_DIR.glob(\"*.png\"))\n",
    "info(f\"Total figures in {FIGS_DIR}: {len(existing_figs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25cc157d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Generating report.md ...\n",
      "[INFO] Report written to homeworks\\HW07\\report.md\n",
      "[INFO] HW07 script finished. Artifacts (metrics, configs, labels, figures) and report are in homeworks/HW07/artifacts/ and homeworks/HW07/report.md\n"
     ]
    }
   ],
   "source": [
    "# 6. Generate report.md inside the script\n",
    "info(\"Generating report.md ...\")\n",
    "report_lines = []\n",
    "report_lines.append(\"# HW07 – Report\\n\")\n",
    "report_lines.append(\"## 1. Datasets\\n\")\n",
    "if not processed_datasets:\n",
    "    report_lines.append(\"- No datasets processed. Check DATA_FILES paths.\\n\")\n",
    "else:\n",
    "    for ds_name in processed_datasets:\n",
    "        # attempt to get metadata from metrics_summary\n",
    "        ms = metrics_summary.get(ds_name, {})\n",
    "        # basic info\n",
    "        try:\n",
    "            df_tmp = pd.read_csv(DATA_DIR / f\"{ds_name}.csv\")\n",
    "            rows, cols = df_tmp.shape\n",
    "            cols_minus_id = cols - (1 if \"sample_id\" in df_tmp.columns else 0)\n",
    "            dtypes = df_tmp.dtypes.value_counts().to_dict()\n",
    "            missing_info = df_tmp.isna().sum()\n",
    "            missing_count = int((missing_info > 0).sum())\n",
    "        except Exception:\n",
    "            rows = cols_minus_id = None\n",
    "            dtypes = {}\n",
    "            missing_count = None\n",
    "        report_lines.append(f\"### Dataset `{ds_name}`\\n\")\n",
    "        report_lines.append(f\"- File: `data/{ds_name}.csv`\\n\")\n",
    "        if rows is not None:\n",
    "            report_lines.append(f\"- Size: {rows} rows, {cols_minus_id} features (excluding sample_id)\\n\")\n",
    "        report_lines.append(f\"- Feature dtypes counts (approx): {dtypes}\\n\")\n",
    "        report_lines.append(f\"- Missing columns count: {missing_count}\\n\")\n",
    "        # chosen config\n",
    "        chosen = ms.get(\"chosen\", {})\n",
    "        if chosen:\n",
    "            report_lines.append(f\"- Chosen best algorithm: **{chosen.get('algo')}** with params `{chosen.get('params')}`\\n\")\n",
    "            report_lines.append(f\"- Chosen metrics (internal): {chosen.get('metrics')}\\n\")\n",
    "        report_lines.append(\"\\n\")\n",
    "\n",
    "report_lines.append(\"## 2. Protocol\\n\")\n",
    "report_lines.append(\"- Preprocessing: SimpleImputer (mean) for numeric, OneHotEncoder for categorical (when present), StandardScaler applied after transformations.\\n\")\n",
    "report_lines.append(\"- KMeans: searched k in a reasonable range (2..12 or adaptive), n_init=10, random_state fixed.\\n\")\n",
    "report_lines.append(\"- DBSCAN: eps grid heuristics based on kNN distances, min_samples=5. For DBSCAN, metrics were computed on non-noise points.\\n\")\n",
    "report_lines.append(\"- Metrics: silhouette (primary), Davies-Bouldin (lower better), Calinski-Harabasz (higher better).\\n\")\n",
    "report_lines.append(\"- Visualization: PCA(2D) scatter for best solution per dataset. t-SNE optionally and not used by default.\\n\")\n",
    "\n",
    "report_lines.append(\"\\n## 3. Models\\n\")\n",
    "report_lines.append(\"- Per dataset we compared: KMeans and DBSCAN (fallback to Agglomerative if DBSCAN unsuitable).\\n\")\n",
    "report_lines.append(\"- Parameter grids and selection heuristics saved to artifacts/best_configs.json.\\n\")\n",
    "\n",
    "report_lines.append(\"\\n## 4. Results\\n\")\n",
    "for ds_name in processed_datasets:\n",
    "    ms = metrics_summary.get(ds_name, {})\n",
    "    report_lines.append(f\"### {ds_name}\\n\")\n",
    "    report_lines.append(\"- KMeans summary:\\n\")\n",
    "    krec = ms.get(\"kmeans\", {})\n",
    "    report_lines.append(f\"  - params: {krec.get('params')}\\n\")\n",
    "    report_lines.append(f\"  - metrics: {krec.get('metrics')}\\n\")\n",
    "    report_lines.append(\"- Second algorithm summary:\\n\")\n",
    "    srec = ms.get(\"second\", {})\n",
    "    report_lines.append(f\"  - algo: {srec.get('algo')}\\n\")\n",
    "    report_lines.append(f\"  - params: {srec.get('params')}\\n\")\n",
    "    report_lines.append(f\"  - metrics: {srec.get('metrics')}\\n\")\n",
    "    report_lines.append(f\"  - noise_frac (if DBSCAN): {srec.get('noise_frac')}\\n\")\n",
    "    report_lines.append(f\"- Chosen: {ms.get('chosen')}\\n\")\n",
    "    report_lines.append(\"\\n\")\n",
    "\n",
    "report_lines.append(\"## 5. Analysis\\n\")\n",
    "report_lines.append(\"- Observations: KMeans works well for spherical, evenly scaled clusters. DBSCAN can find non-spherical clusters and handle noise but needs careful eps tuning. Agglomerative can be robust for hierarchical structure.\\n\")\n",
    "report_lines.append(\"- Impact factors: scaling, presence of outliers/noise, different densities, and categorical features (if present) affect distance-based clustering strongly.\\n\")\n",
    "report_lines.append(\"- Stability checks: run KMeans multiple times with different seeds; consider ARI/VI metrics to compare runs (not computed here by default).\\n\")\n",
    "\n",
    "report_lines.append(\"\\n## 6. Conclusion\\n\")\n",
    "report_lines.append(\"- Use scaling before KMeans and Agglomerative.\\n\")\n",
    "report_lines.append(\"- DBSCAN is valuable when clusters are irregular but requires eps tuning and handling of noise.\\n\")\n",
    "report_lines.append(\"- Internal metrics must be interpreted together (silhouette + DB + CH) and balanced with visual inspection (PCA plots).\\n\")\n",
    "report_lines.append(\"- Save labels and configs for reproducibility (done in artifacts/).\\n\")\n",
    "\n",
    "report_path = BASE_DIR / \"report.md\"\n",
    "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(report_lines))\n",
    "info(f\"Report written to {report_path}\")\n",
    "\n",
    "info(\"HW07 script finished. Artifacts (metrics, configs, labels, figures) and report are in homeworks/HW07/artifacts/ and homeworks/HW07/report.md\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
