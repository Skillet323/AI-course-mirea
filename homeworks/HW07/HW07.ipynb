{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1259d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports & Settings\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    davies_bouldin_score,\n",
    "    calinski_harabasz_score,\n",
    ")\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path(\".\")\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "ARTIFACTS_DIR = BASE_DIR / \"artifacts\"\n",
    "FIGS_DIR = ARTIFACTS_DIR / \"figures\"\n",
    "LABELS_DIR = ARTIFACTS_DIR / \"labels\"\n",
    "for p in (ARTIFACTS_DIR, FIGS_DIR, LABELS_DIR):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DEFAULT_DATA_FILES = [\n",
    "    DATA_DIR / \"S07-hw-dataset-02.csv\",\n",
    "    DATA_DIR / \"S07-hw-dataset-03.csv\",\n",
    "    DATA_DIR / \"S07-hw-dataset-04.csv\",\n",
    "]\n",
    "\n",
    "DATA_FILES = DEFAULT_DATA_FILES\n",
    "\n",
    "# Helper printing\n",
    "def info(msg: str):\n",
    "    print(f\"[INFO] {msg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20d87e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Utility functions\n",
    "def safe_read_csv(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Dataset file not found: {path}\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def build_preprocessor(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Build ColumnTransformer:\n",
    "    - Numeric: SimpleImputer(mean) -> StandardScaler\n",
    "    - Categorical: SimpleImputer(constant) -> OneHotEncoder(handle_unknown='ignore')\n",
    "    Returns transformer and list of numeric/categorical column names.\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    # exclude sample_id if present\n",
    "    numeric_cols = [c for c in numeric_cols if c != \"sample_id\"]\n",
    "    cat_cols = [c for c in df.columns if c not in numeric_cols and c != \"sample_id\"]\n",
    "    # numeric pipeline\n",
    "    numeric_transformer = (\n",
    "        (\"num\", SimpleImputer(strategy=\"mean\"), numeric_cols)\n",
    "        if numeric_cols\n",
    "        else None\n",
    "    )\n",
    "    # categorical pipeline\n",
    "    categorical_transformer = (\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols)\n",
    "        if cat_cols\n",
    "        else None\n",
    "    )\n",
    "    transformers = []\n",
    "    if numeric_transformer:\n",
    "        transformers.append(numeric_transformer)\n",
    "    if categorical_transformer:\n",
    "        transformers.append(categorical_transformer)\n",
    "    # Build ColumnTransformer that imputes and encodes; scaling applied after transform\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "\n",
    "    ct = ColumnTransformer(transformers, remainder=\"drop\", sparse_threshold=0)\n",
    "    return ct, numeric_cols, cat_cols\n",
    "\n",
    "def preprocess_fit_transform(ct: ColumnTransformer, df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Fit ColumnTransformer on df and return transformed numpy array and feature names.\n",
    "    Then apply StandardScaler to numeric+encoded features.\n",
    "    \"\"\"\n",
    "    X_pre = ct.fit_transform(df)\n",
    "    # Build feature names for transformed output (best-effort)\n",
    "    feat_names = []\n",
    "    for name, transformer, cols in ct.transformers_:\n",
    "        if transformer is None:\n",
    "            continue\n",
    "        if hasattr(transformer, \"get_feature_names_out\"):\n",
    "            try:\n",
    "                out_names = transformer.get_feature_names_out(cols)\n",
    "            except Exception:\n",
    "                out_names = cols\n",
    "        else:\n",
    "            out_names = cols\n",
    "        feat_names.extend([str(n) for n in out_names])\n",
    "    # scale\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_pre)\n",
    "    return X_scaled, feat_names, ct, scaler\n",
    "\n",
    "def preprocess_transform(ct: ColumnTransformer, scaler: StandardScaler, df: pd.DataFrame):\n",
    "    X_pre = ct.transform(df)\n",
    "    X_scaled = scaler.transform(X_pre)\n",
    "    return X_scaled\n",
    "\n",
    "def compute_internal_metrics(X: np.ndarray, labels: np.ndarray, consider_noise_as_cluster=False):\n",
    "    \"\"\"\n",
    "    Compute silhouette, davies_bouldin, calinski_harabasz.\n",
    "    For labels where one cluster or all noise, return None appropriately.\n",
    "    \"\"\"\n",
    "    metrics = {\"silhouette\": None, \"davies_bouldin\": None, \"calinski_harabasz\": None}\n",
    "    # number of valid clusters\n",
    "    unique_labels = np.unique(labels)\n",
    "    # If DBSCAN noise present and consider_noise_as_cluster False, we compute metrics on non-noise only\n",
    "    if -1 in unique_labels and (not consider_noise_as_cluster):\n",
    "        non_noise_mask = labels != -1\n",
    "        if non_noise_mask.sum() < 2:\n",
    "            return metrics, {\"n_points_non_noise\": int(non_noise_mask.sum()), \"n_clusters_non_noise\": 0}\n",
    "        X_eval = X[non_noise_mask]\n",
    "        labels_eval = labels[non_noise_mask]\n",
    "    else:\n",
    "        X_eval = X\n",
    "        labels_eval = labels\n",
    "    n_clusters = len(set(labels_eval))\n",
    "    if n_clusters <= 1 or X_eval.shape[0] <= 1:\n",
    "        return metrics, {\"n_points_non_noise\": int((labels != -1).sum()), \"n_clusters_non_noise\": n_clusters}\n",
    "    try:\n",
    "        metrics[\"silhouette\"] = float(silhouette_score(X_eval, labels_eval))\n",
    "    except Exception:\n",
    "        metrics[\"silhouette\"] = None\n",
    "    try:\n",
    "        metrics[\"davies_bouldin\"] = float(davies_bouldin_score(X_eval, labels_eval))\n",
    "    except Exception:\n",
    "        metrics[\"davies_bouldin\"] = None\n",
    "    try:\n",
    "        metrics[\"calinski_harabasz\"] = float(calinski_harabasz_score(X_eval, labels_eval))\n",
    "    except Exception:\n",
    "        metrics[\"calinski_harabasz\"] = None\n",
    "    return metrics, {\"n_points_non_noise\": int((labels != -1).sum()), \"n_clusters_non_noise\": n_clusters}\n",
    "\n",
    "def pca_2d(X: np.ndarray, random_state=RANDOM_STATE):\n",
    "    pca = PCA(n_components=2, random_state=random_state)\n",
    "    X2 = pca.fit_transform(X)\n",
    "    return X2, pca\n",
    "\n",
    "def save_fig(fig, path: Path):\n",
    "    fig.savefig(path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0af3f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing dataset 1: data\\S07-hw-dataset-02.csv\n",
      "[INFO] Basic EDA:\n",
      "[INFO]  - shape: (8000, 4)\n",
      "[INFO]  - head:\n",
      " sample_id        x1        x2    z_noise\n",
      "         0  0.098849 -1.846034  21.288122\n",
      "         1 -1.024516  1.829616   6.072952\n",
      "         2 -1.094178 -0.158545 -18.938342\n",
      "         3 -1.612808 -1.565844 -11.629462\n",
      "         4  1.659901 -2.133292   1.895472\n",
      "[INFO]  - dtypes:\n",
      "sample_id      int64\n",
      "x1           float64\n",
      "x2           float64\n",
      "z_noise      float64\n",
      "dtype: object\n",
      "[INFO]  - missing summary (top):\n",
      "Series([], )\n",
      "[INFO] Building preprocessor (imputation, encoding)...\n",
      "[INFO]  - transformed shape: (8000, 3)\n",
      "[INFO] Searching K for KMeans (2..12)\n",
      "[INFO] Saved figure artifacts\\figures\\S07-hw-dataset-02_silhouette_vs_k.png\n",
      "[INFO] Selected KMeans k = 2\n",
      "[INFO] Trying DBSCAN parameter sweep (eps grid)\n",
      "[INFO] Saved figure artifacts\\figures\\S07-hw-dataset-02_dbscan_silhouette_vs_eps.png\n",
      "[INFO] Selected DBSCAN eps = 0.34605338681853065\n",
      "[INFO] Chosen best for S07-hw-dataset-02: KMeans, params={'k': 2}, metrics={'silhouette': 0.3068610017701601, 'davies_bouldin': 1.3234721699867644, 'calinski_harabasz': 3573.3933329348392}\n",
      "[INFO] Saved PCA scatter to artifacts\\figures\\S07-hw-dataset-02_pca2d_KMeans.png\n",
      "[INFO] Saved labels to artifacts\\labels\\labels_S07-hw-dataset-02.csv\n",
      "[INFO] Processing dataset 2: data\\S07-hw-dataset-03.csv\n",
      "[INFO] Basic EDA:\n",
      "[INFO]  - shape: (15000, 5)\n",
      "[INFO]  - head:\n",
      " sample_id        x1        x2    f_corr   f_noise\n",
      "         0 -2.710470  4.997107 -1.015703  0.718508\n",
      "         1  8.730238 -8.787416  3.953063 -1.105349\n",
      "         2 -1.079600 -2.558708  0.976628 -3.605776\n",
      "         3  6.854042  1.560181  1.760614 -1.230946\n",
      "         4  9.963812 -8.869921  2.966583  0.915899\n",
      "[INFO]  - dtypes:\n",
      "sample_id      int64\n",
      "x1           float64\n",
      "x2           float64\n",
      "f_corr       float64\n",
      "f_noise      float64\n",
      "dtype: object\n",
      "[INFO]  - missing summary (top):\n",
      "Series([], )\n",
      "[INFO] Building preprocessor (imputation, encoding)...\n",
      "[INFO]  - transformed shape: (15000, 4)\n",
      "[INFO] Searching K for KMeans (2..12)\n",
      "[INFO] Saved figure artifacts\\figures\\S07-hw-dataset-03_silhouette_vs_k.png\n",
      "[INFO] Selected KMeans k = 3\n",
      "[INFO] Trying DBSCAN parameter sweep (eps grid)\n",
      "[INFO] Saved figure artifacts\\figures\\S07-hw-dataset-03_dbscan_silhouette_vs_eps.png\n",
      "[INFO] Selected DBSCAN eps = 0.615039098413056\n",
      "[INFO] Chosen best for S07-hw-dataset-03: KMeans, params={'k': 3}, metrics={'silhouette': 0.31554470037825183, 'davies_bouldin': 1.1577256320598661, 'calinski_harabasz': 6957.162639510166}\n",
      "[INFO] Saved PCA scatter to artifacts\\figures\\S07-hw-dataset-03_pca2d_KMeans.png\n",
      "[INFO] Saved labels to artifacts\\labels\\labels_S07-hw-dataset-03.csv\n",
      "[INFO] Processing dataset 3: data\\S07-hw-dataset-04.csv\n",
      "[INFO] Basic EDA:\n",
      "[INFO]  - shape: (10000, 33)\n",
      "[INFO]  - head:\n",
      " sample_id cat_a cat_b        n01        n02       n03       n04        n05        n06        n07        n08        n09        n10       n11        n12        n13        n14        n15        n16        n17       n18        n19        n20        n21        n22       n23        n24       n25       n26       n27       n28       n29       n30\n",
      "         0     B     X  -4.827501 -24.507466 -7.852963  0.771781  28.297884  -4.493911 -42.769449 -17.172862   7.693372   7.735167  7.090589 -15.806796 -11.578628  23.975877   9.633308 -14.969956 -14.378773       NaN   9.678605   1.296645  24.597176 -26.354320  4.543397 -19.549036 -3.051332 -5.538587 -3.084457  5.499629 -6.128896  3.132067\n",
      "         1     F     V  51.302500        NaN  5.534737 51.305464  -8.027553  28.297548        NaN -18.190684  41.217407  12.091455 -1.781172  -9.555944 -45.541130  22.061363  -1.247411  30.652032  26.887402 -7.090226 -11.740552 -20.841609 -18.216260   8.527932 17.202115 -30.452260  0.855326  1.199066  3.597555 -2.239703  2.932710  0.473145\n",
      "         2     A     W  -4.820828  -2.625385 27.891578  1.523041  -5.776687 -16.298523   2.462937   5.698795 -11.771789 -35.640006 -9.769188  27.225550  11.386833 -14.798406 -24.153257  -8.115439  37.827212  9.182300  10.406210  19.959493 -48.260775   9.313232 12.323411  55.081325 -3.945606 -0.280540 -0.130583 -7.353205 -2.942836  1.460477\n",
      "         3     B     X  -2.627573 -25.063639 -9.450011 -8.344669  22.371118 -11.525848 -43.762607  -7.063217   6.292940   4.398893 -4.295396 -17.219277  -4.858199   8.202211   1.654673 -11.684173 -16.285629 -3.188826   5.671305  10.111227  24.700663 -25.466915 -3.398665 -18.174541  0.438229  3.152556  3.859283 -2.678769 -2.213923 -4.724639\n",
      "         4     C     Y -11.415710  -8.692169 48.636163 14.661826 -39.634618  10.769075  40.187536  -7.420309  15.166054  -0.593148 -1.356762 -20.186909  33.010555  -3.255715  19.503501   0.745787   0.790124 14.494466 -15.670157  10.059768 -79.710383 -13.694253 41.575892  -9.498640  1.529608 -1.641347  3.500090  3.111257  1.475232 -1.321676\n",
      "[INFO]  - dtypes:\n",
      "sample_id      int64\n",
      "cat_a         object\n",
      "cat_b         object\n",
      "n01          float64\n",
      "n02          float64\n",
      "n03          float64\n",
      "n04          float64\n",
      "n05          float64\n",
      "n06          float64\n",
      "n07          float64\n",
      "n08          float64\n",
      "n09          float64\n",
      "n10          float64\n",
      "n11          float64\n",
      "n12          float64\n",
      "n13          float64\n",
      "n14          float64\n",
      "n15          float64\n",
      "n16          float64\n",
      "n17          float64\n",
      "n18          float64\n",
      "n19          float64\n",
      "n20          float64\n",
      "n21          float64\n",
      "n22          float64\n",
      "n23          float64\n",
      "n24          float64\n",
      "n25          float64\n",
      "n26          float64\n",
      "n27          float64\n",
      "n28          float64\n",
      "n29          float64\n",
      "n30          float64\n",
      "dtype: object\n",
      "[INFO]  - missing summary (top):\n",
      "n26    224\n",
      "n21    215\n",
      "n18    212\n",
      "n17    212\n",
      "n28    211\n",
      "[INFO] Building preprocessor (imputation, encoding)...\n",
      "[INFO]  - transformed shape: (10000, 42)\n",
      "[INFO] Searching K for KMeans (2..12)\n",
      "[INFO] Saved figure artifacts\\figures\\S07-hw-dataset-04_silhouette_vs_k.png\n",
      "[INFO] Selected KMeans k = 6\n",
      "[INFO] Trying DBSCAN parameter sweep (eps grid)\n",
      "[INFO] Saved figure artifacts\\figures\\S07-hw-dataset-04_dbscan_silhouette_vs_eps.png\n",
      "[INFO] Selected DBSCAN eps = 3.2615185950303487\n",
      "[INFO] Chosen best for S07-hw-dataset-04: KMeans, params={'k': 6}, metrics={'silhouette': 0.429013016033581, 'davies_bouldin': 0.9830409608302536, 'calinski_harabasz': 4681.459777999189}\n",
      "[INFO] Saved PCA scatter to artifacts\\figures\\S07-hw-dataset-04_pca2d_KMeans.png\n",
      "[INFO] Saved labels to artifacts\\labels\\labels_S07-hw-dataset-04.csv\n"
     ]
    }
   ],
   "source": [
    "# 3. Main experiment per dataset\n",
    "metrics_summary = {}\n",
    "best_configs = {}\n",
    "\n",
    "processed_datasets = []  # store dataset names processed\n",
    "\n",
    "for idx, data_file in enumerate(DATA_FILES, start=1):\n",
    "    dataset_name = data_file.stem\n",
    "    info(f\"Processing dataset {idx}: {data_file}\")\n",
    "    if not data_file.exists():\n",
    "        info(f\"File {data_file} not found — skipping\")\n",
    "        continue\n",
    "    df = safe_read_csv(data_file)\n",
    "    processed_datasets.append(dataset_name)\n",
    "\n",
    "    info(\"Basic EDA:\")\n",
    "    info(f\" - shape: {df.shape}\")\n",
    "    info(f\" - head:\\n{df.head().to_string(index=False)}\")\n",
    "    info(f\" - dtypes:\\n{df.dtypes}\")\n",
    "    # missing\n",
    "    miss = df.isna().sum()\n",
    "    info(f\" - missing summary (top):\\n{miss[miss>0].sort_values(ascending=False).head().to_string() or 'No missing values'}\")\n",
    "\n",
    "    # Separate sample_id if exists\n",
    "    sample_ids = df[\"sample_id\"] if \"sample_id\" in df.columns else pd.Series(np.arange(len(df)), name=\"sample_id\")\n",
    "    X_df = df.drop(columns=[\"sample_id\"]) if \"sample_id\" in df.columns else df.copy()\n",
    "\n",
    "    info(\"Building preprocessor (imputation, encoding)...\")\n",
    "    ct, numeric_cols, cat_cols = build_preprocessor(X_df)\n",
    "    try:\n",
    "        X_scaled, feat_names, ct_fitted, scaler_fitted = preprocess_fit_transform(ct, X_df)\n",
    "        info(f\" - transformed shape: {X_scaled.shape}\")\n",
    "    except Exception as exc:\n",
    "        info(f\"Preprocessing failed: {exc}\")\n",
    "        continue\n",
    "\n",
    "    info(\"Searching K for KMeans (2..12)\")\n",
    "    k_range = list(range(2, min(13, max(3, X_scaled.shape[0]//5))))  # adaptive upper bound\n",
    "    k_range = k_range if k_range else [2,3,4]\n",
    "    k_metrics = []\n",
    "    for k in k_range:\n",
    "        try:\n",
    "            km = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=10)\n",
    "            labels_k = km.fit_predict(X_scaled)\n",
    "            met, extra = compute_internal_metrics(X_scaled, labels_k, consider_noise_as_cluster=True)\n",
    "            k_metrics.append({\"k\": k, **met, \"n_clusters\": len(set(labels_k))})\n",
    "        except Exception as exc:\n",
    "            k_metrics.append({\"k\": k, \"silhouette\": None, \"davies_bouldin\": None, \"calinski_harabasz\": None, \"n_clusters\": None})\n",
    "    kdf = pd.DataFrame(k_metrics)\n",
    "\n",
    "    # Plot silhouette vs k\n",
    "    try:\n",
    "        fig, ax = plt.subplots(figsize=(6,4))\n",
    "        ax.plot(kdf[\"k\"], kdf[\"silhouette\"], marker=\"o\")\n",
    "        ax.set_xlabel(\"k (KMeans)\")\n",
    "        ax.set_ylabel(\"Silhouette score\")\n",
    "        ax.set_title(f\"{dataset_name}: Silhouette vs k (KMeans)\")\n",
    "        path = FIGS_DIR / f\"{dataset_name}_silhouette_vs_k.png\"\n",
    "        save_fig(fig, path)\n",
    "        info(f\"Saved figure {path}\")\n",
    "    except Exception as exc:\n",
    "        info(f\"Failed to plot silhouette vs k: {exc}\")\n",
    "\n",
    "    # Choose best k by silhouette (primary), break ties by calinski_harabasz\n",
    "    kdf_valid = kdf.dropna(subset=[\"silhouette\"])\n",
    "    if not kdf_valid.empty:\n",
    "        best_k_row = kdf_valid.sort_values([\"silhouette\", \"calinski_harabasz\"], ascending=[False, False]).iloc[0]\n",
    "        best_k = int(best_k_row[\"k\"])\n",
    "    else:\n",
    "        best_k = int(k_range[0])\n",
    "    info(f\"Selected KMeans k = {best_k}\")\n",
    "\n",
    "    best_kmeans = KMeans(n_clusters=best_k, random_state=RANDOM_STATE, n_init=10)\n",
    "    labels_kmeans = best_kmeans.fit_predict(X_scaled)\n",
    "    metrics_kmeans, extra_kmeans = compute_internal_metrics(X_scaled, labels_kmeans, consider_noise_as_cluster=True)\n",
    "\n",
    "    # We'll attempt DBSCAN with eps grid; if DBSCAN produces too much noise/unusable, we'll fallback to Agglomerative\n",
    "    info(\"Trying DBSCAN parameter sweep (eps grid)\")\n",
    "    # Use heuristic eps range based on pairwise distances median (approx) - use kNN distances would be better but keep simple\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "    try:\n",
    "        # estimate a scale: distance to 5th neighbor median\n",
    "        neigh = NearestNeighbors(n_neighbors=min(10, max(2, X_scaled.shape[0]//10))).fit(X_scaled)\n",
    "        dists, _ = neigh.kneighbors(X_scaled)\n",
    "        # use distance to 4th or median of 5th neighbor if available\n",
    "        kth = min(4, dists.shape[1]-1)\n",
    "        kth_dists = dists[:, kth]\n",
    "        median_kth = float(np.median(kth_dists))\n",
    "        eps_vals = np.unique(np.concatenate([np.linspace(median_kth*0.3, median_kth*1.5, 8), np.linspace(median_kth*0.1, median_kth*3.0, 6)]))\n",
    "    except Exception:\n",
    "        eps_vals = np.linspace(0.1, 2.0, 8)\n",
    "\n",
    "    dbscan_metrics = []\n",
    "    for eps in eps_vals:\n",
    "        try:\n",
    "            db = DBSCAN(eps=float(eps), min_samples=5)\n",
    "            labels_db = db.fit_predict(X_scaled)\n",
    "            # compute noise fraction\n",
    "            noise_frac = float((labels_db == -1).sum()) / len(labels_db)\n",
    "            # compute metrics on non-noise points if at least 2 clusters\n",
    "            met, extra = compute_internal_metrics(X_scaled, labels_db, consider_noise_as_cluster=False)\n",
    "            dbscan_metrics.append({\"eps\": float(eps), \"noise_frac\": noise_frac, **met, \"n_clusters_non_noise\": extra[\"n_clusters_non_noise\"]})\n",
    "        except Exception:\n",
    "            dbscan_metrics.append({\"eps\": float(eps), \"noise_frac\": None, \"silhouette\": None, \"davies_bouldin\": None, \"calinski_harabasz\": None, \"n_clusters_non_noise\": None})\n",
    "    dbdf = pd.DataFrame(dbscan_metrics)\n",
    "\n",
    "    # Plot silhouette vs eps (for DBSCAN non-noise)\n",
    "    try:\n",
    "        fig, ax = plt.subplots(figsize=(6,4))\n",
    "        ax.plot(dbdf[\"eps\"], dbdf[\"silhouette\"], marker=\"o\")\n",
    "        ax.set_xlabel(\"eps (DBSCAN)\")\n",
    "        ax.set_ylabel(\"Silhouette (non-noise)\")\n",
    "        ax.set_title(f\"{dataset_name}: DBSCAN silhouette vs eps\")\n",
    "        path = FIGS_DIR / f\"{dataset_name}_dbscan_silhouette_vs_eps.png\"\n",
    "        save_fig(fig, path)\n",
    "        info(f\"Saved figure {path}\")\n",
    "    except Exception as exc:\n",
    "        info(f\"Failed to plot DBSCAN silhouette vs eps: {exc}\")\n",
    "\n",
    "    # Choose best DBSCAN by silhouette for non-noise points, but also prefer low noise fraction (<0.5)\n",
    "    dbdf_valid = dbdf.dropna(subset=[\"silhouette\"])\n",
    "    dbdf_valid_filtered = dbdf_valid[dbdf_valid[\"noise_frac\"] <= 0.5] if not dbdf_valid.empty else dbdf_valid\n",
    "    if not dbdf_valid_filtered.empty:\n",
    "        best_db_row = dbdf_valid_filtered.sort_values([\"silhouette\", \"calinski_harabasz\"], ascending=[False, False]).iloc[0]\n",
    "        best_eps = float(best_db_row[\"eps\"])\n",
    "    elif not dbdf_valid.empty:\n",
    "        best_db_row = dbdf_valid.sort_values([\"silhouette\", \"calinski_harabasz\"], ascending=[False, False]).iloc[0]\n",
    "        best_eps = float(best_db_row[\"eps\"])\n",
    "    else:\n",
    "        best_eps = None\n",
    "\n",
    "    if best_eps is not None:\n",
    "        info(f\"Selected DBSCAN eps = {best_eps}\")\n",
    "        best_db = DBSCAN(eps=best_eps, min_samples=5)\n",
    "        labels_db_final = best_db.fit_predict(X_scaled)\n",
    "        metrics_db, extra_db = compute_internal_metrics(X_scaled, labels_db_final, consider_noise_as_cluster=False)\n",
    "    else:\n",
    "        info(\"DBSCAN did not produce valid clustering; falling back to AgglomerativeClustering\")\n",
    "        # Agglomerative: try linkage variants and a few k values\n",
    "        aggl_metrics = []\n",
    "        for linkage in [\"ward\", \"complete\", \"average\"]:\n",
    "            # ward requires euclidean and no categorical encoding issues (we have numeric scaled features)\n",
    "            for k in range(2, min(8, X_scaled.shape[0]//5 + 3)):\n",
    "                try:\n",
    "                    agg = AgglomerativeClustering(n_clusters=k, linkage=linkage)\n",
    "                    labels_agg = agg.fit_predict(X_scaled)\n",
    "                    met, extra = compute_internal_metrics(X_scaled, labels_agg, consider_noise_as_cluster=True)\n",
    "                    aggl_metrics.append({\"linkage\": linkage, \"k\": k, **met})\n",
    "                except Exception:\n",
    "                    aggl_metrics.append({\"linkage\": linkage, \"k\": k, \"silhouette\": None, \"davies_bouldin\": None, \"calinski_harabasz\": None})\n",
    "        aggl_df = pd.DataFrame(aggl_metrics)\n",
    "        aggl_valid = aggl_df.dropna(subset=[\"silhouette\"])\n",
    "        if not aggl_valid.empty:\n",
    "            best_aggl = aggl_valid.sort_values([\"silhouette\", \"calinski_harabasz\"], ascending=[False, False]).iloc[0]\n",
    "            best_linkage = best_aggl[\"linkage\"]\n",
    "            best_k_aggl = int(best_aggl[\"k\"])\n",
    "        else:\n",
    "            best_linkage = \"ward\"\n",
    "            best_k_aggl = 2\n",
    "        info(f\"Selected Agglomerative linkage={best_linkage}, k={best_k_aggl}\")\n",
    "        best_agg_model = AgglomerativeClustering(n_clusters=best_k_aggl, linkage=best_linkage)\n",
    "        labels_db_final = best_agg_model.fit_predict(X_scaled)\n",
    "        metrics_db, extra_db = compute_internal_metrics(X_scaled, labels_db_final, consider_noise_as_cluster=True)\n",
    "\n",
    "    # We pick best among KMeans and (DBSCAN or Aggl) by primary metric silhouette (non-noise for DBSCAN)\n",
    "    # Prepare metric records\n",
    "    record = {}\n",
    "    record[\"kmeans\"] = {\"params\": {\"k\": best_k}, \"metrics\": metrics_kmeans, \"n_clusters\": int(len(set(labels_kmeans)))}\n",
    "    record[\"second\"] = {}\n",
    "    if best_eps is not None:\n",
    "        record[\"second\"][\"algo\"] = \"DBSCAN\"\n",
    "        record[\"second\"][\"params\"] = {\"eps\": best_eps, \"min_samples\": 5}\n",
    "        record[\"second\"][\"metrics\"] = metrics_db\n",
    "        record[\"second\"][\"noise_frac\"] = float((labels_db_final == -1).sum()) / len(labels_db_final)\n",
    "        n_clusters_second = int(len(set(labels_db_final)) - (1 if -1 in labels_db_final else 0))\n",
    "    else:\n",
    "        record[\"second\"][\"algo\"] = \"Agglomerative\"\n",
    "        record[\"second\"][\"params\"] = {\"linkage\": best_linkage, \"k\": best_k_aggl}\n",
    "        record[\"second\"][\"metrics\"] = metrics_db\n",
    "        record[\"second\"][\"noise_frac\"] = None\n",
    "        n_clusters_second = int(len(set(labels_db_final)))\n",
    "\n",
    "    # Compare silhouette (note: DBSCAN metrics computed on non-noise) -> choose best\n",
    "    s_k = record[\"kmeans\"][\"metrics\"].get(\"silhouette\")\n",
    "    s_s = record[\"second\"][\"metrics\"].get(\"silhouette\")\n",
    "    # For cases where silhouette None, use calinski_harabasz as fallback (higher better)\n",
    "    def pick_best(s1, s2, m1, m2):\n",
    "        if s1 is not None and s2 is not None:\n",
    "            return \"kmeans\" if s1 >= s2 else \"second\"\n",
    "        if s1 is not None:\n",
    "            return \"kmeans\"\n",
    "        if s2 is not None:\n",
    "            return \"second\"\n",
    "        # fallback to ch\n",
    "        ch1 = m1.get(\"calinski_harabasz\")\n",
    "        ch2 = m2.get(\"calinski_harabasz\")\n",
    "        if ch1 is not None and ch2 is not None:\n",
    "            return \"kmeans\" if ch1 >= ch2 else \"second\"\n",
    "        return \"kmeans\"\n",
    "\n",
    "    best_choice_key = pick_best(\n",
    "        s_k, s_s, record[\"kmeans\"][\"metrics\"], record[\"second\"][\"metrics\"]\n",
    "    )\n",
    "    if best_choice_key == \"kmeans\":\n",
    "        chosen_algo = \"KMeans\"\n",
    "        chosen_labels = labels_kmeans\n",
    "        chosen_params = record[\"kmeans\"][\"params\"]\n",
    "        chosen_metrics = record[\"kmeans\"][\"metrics\"]\n",
    "    else:\n",
    "        chosen_algo = record[\"second\"][\"algo\"]\n",
    "        chosen_labels = labels_db_final\n",
    "        chosen_params = record[\"second\"][\"params\"]\n",
    "        chosen_metrics = record[\"second\"][\"metrics\"]\n",
    "\n",
    "    info(f\"Chosen best for {dataset_name}: {chosen_algo}, params={chosen_params}, metrics={chosen_metrics}\")\n",
    "\n",
    "    try:\n",
    "        X2, pca_model = pca_2d(X_scaled)\n",
    "        # scatter colored by cluster\n",
    "        unique_lbls = np.unique(chosen_labels)\n",
    "        palette = plt.get_cmap(\"tab20\")\n",
    "        fig, ax = plt.subplots(figsize=(6,5))\n",
    "        for i, lbl in enumerate(unique_lbls):\n",
    "            mask = chosen_labels == lbl\n",
    "            ax.scatter(\n",
    "                X2[mask, 0],\n",
    "                X2[mask, 1],\n",
    "                s=20,\n",
    "                alpha=0.8,\n",
    "                label=f\"cluster {int(lbl)}\" if lbl != -1 else \"noise (-1)\",\n",
    "                color=palette(i % 20),\n",
    "                edgecolors=\"k\" if chosen_algo != \"DBSCAN\" else None,\n",
    "                linewidths=0.2,\n",
    "            )\n",
    "        ax.set_title(f\"{dataset_name} PCA(2D) - {chosen_algo}\")\n",
    "        ax.legend(loc=\"best\", fontsize=\"small\", markerscale=2)\n",
    "        ax.set_xlabel(\"PC1\")\n",
    "        ax.set_ylabel(\"PC2\")\n",
    "        path = FIGS_DIR / f\"{dataset_name}_pca2d_{chosen_algo}.png\"\n",
    "        save_fig(fig, path)\n",
    "        info(f\"Saved PCA scatter to {path}\")\n",
    "    except Exception as exc:\n",
    "        info(f\"Failed PCA visualization: {exc}\")\n",
    "\n",
    "    labels_out = pd.DataFrame({\"sample_id\": sample_ids.values, \"cluster_label\": chosen_labels})\n",
    "    labels_csv_path = LABELS_DIR / f\"labels_{dataset_name}.csv\"\n",
    "    labels_out.to_csv(labels_csv_path, index=False)\n",
    "    info(f\"Saved labels to {labels_csv_path}\")\n",
    "\n",
    "    metrics_summary[dataset_name] = {\n",
    "        \"kmeans\": record[\"kmeans\"],\n",
    "        \"second\": record[\"second\"],\n",
    "        \"chosen\": {\"algo\": chosen_algo, \"params\": chosen_params, \"metrics\": chosen_metrics},\n",
    "    }\n",
    "    best_configs[dataset_name] = {\"algo\": chosen_algo, \"params\": chosen_params, \"selection_reason\": \"silhouette/CH heuristics\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf1f9478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Writing summary artifacts (metrics_summary.json, best_configs.json)...\n",
      "[INFO] Artifact JSONs written.\n"
     ]
    }
   ],
   "source": [
    "# 4. Save artifacts JSONs\n",
    "info(\"Writing summary artifacts (metrics_summary.json, best_configs.json)...\")\n",
    "with open(ARTIFACTS_DIR / \"metrics_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics_summary, f, ensure_ascii=False, indent=2)\n",
    "with open(ARTIFACTS_DIR / \"best_configs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(best_configs, f, ensure_ascii=False, indent=2)\n",
    "info(\"Artifact JSONs written.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55d75a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Total figures in artifacts\\figures: 9\n"
     ]
    }
   ],
   "source": [
    "# 5. Ensure at least 6 figures exist\n",
    "# Count existing figures\n",
    "existing_figs = list(FIGS_DIR.glob(\"*.png\"))\n",
    "if len(existing_figs) < 6:\n",
    "    info(\"Less than 6 figures found — generating additional diagnostic plots to meet artifact requirements.\")\n",
    "    # For each processed dataset, if silhouette vs k or DBSCAN plots exist, copy or regenerate simple histograms\n",
    "    for ds in processed_datasets:\n",
    "        # simple histogram of first two PCA components if available\n",
    "        try:\n",
    "            # load labels file\n",
    "            labf = LABELS_DIR / f\"labels_{ds}.csv\"\n",
    "            if labf.exists():\n",
    "                labdf = pd.read_csv(labf)\n",
    "                # recreate PCA plot if not present\n",
    "                pca_plot = FIGS_DIR / f\"{ds}_pca2d_KMeans.png\"\n",
    "                if not pca_plot.exists():\n",
    "                    # attempt to recreate quick using data\n",
    "                    df = pd.read_csv(DATA_DIR / f\"{ds}.csv\") if (DATA_DIR / f\"{ds}.csv\").exists() else None\n",
    "                    if df is not None:\n",
    "                        # quick preprocessing naive\n",
    "                        if \"sample_id\" in df.columns:\n",
    "                            sid = df[\"sample_id\"]\n",
    "                            Xdf = df.drop(columns=[\"sample_id\"])\n",
    "                        else:\n",
    "                            sid = pd.Series(np.arange(len(df)), name=\"sample_id\")\n",
    "                            Xdf = df.copy()\n",
    "                        # numeric only\n",
    "                        Xnum = Xdf.select_dtypes(include=[np.number]).fillna(0).values\n",
    "                        if Xnum.shape[1] >= 2:\n",
    "                            pca = PCA(n_components=2, random_state=RANDOM_STATE)\n",
    "                            X2 = pca.fit_transform(StandardScaler().fit_transform(Xnum))\n",
    "                            fig, ax = plt.subplots(figsize=(6,4))\n",
    "                            ax.scatter(X2[:,0], X2[:,1], s=10)\n",
    "                            ax.set_title(f\"{ds} quick PCA\")\n",
    "                            save_fig(fig, FIGS_DIR / f\"{ds}_quick_pca.png\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    # refresh existing_figs\n",
    "    existing_figs = list(FIGS_DIR.glob(\"*.png\"))\n",
    "info(f\"Total figures in {FIGS_DIR}: {len(existing_figs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25cc157d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Generating report.md ...\n",
      "[INFO] Report written to report.md\n",
      "[INFO] HW07 script finished. Artifacts (metrics, configs, labels, figures) and report are in homeworks/HW07/artifacts/ and homeworks/HW07/report.md\n"
     ]
    }
   ],
   "source": [
    "# 6. Generate report.md inside the script\n",
    "info(\"Generating report.md ...\")\n",
    "report_lines = []\n",
    "report_lines.append(\"# HW07 – Report\\n\")\n",
    "report_lines.append(\"## 1. Datasets\\n\")\n",
    "if not processed_datasets:\n",
    "    report_lines.append(\"- No datasets processed. Check DATA_FILES paths.\\n\")\n",
    "else:\n",
    "    for ds_name in processed_datasets:\n",
    "        # attempt to get metadata from metrics_summary\n",
    "        ms = metrics_summary.get(ds_name, {})\n",
    "        # basic info\n",
    "        try:\n",
    "            df_tmp = pd.read_csv(DATA_DIR / f\"{ds_name}.csv\")\n",
    "            rows, cols = df_tmp.shape\n",
    "            cols_minus_id = cols - (1 if \"sample_id\" in df_tmp.columns else 0)\n",
    "            dtypes = df_tmp.dtypes.value_counts().to_dict()\n",
    "            missing_info = df_tmp.isna().sum()\n",
    "            missing_count = int((missing_info > 0).sum())\n",
    "        except Exception:\n",
    "            rows = cols_minus_id = None\n",
    "            dtypes = {}\n",
    "            missing_count = None\n",
    "        report_lines.append(f\"### Dataset `{ds_name}`\\n\")\n",
    "        report_lines.append(f\"- File: `data/{ds_name}.csv`\\n\")\n",
    "        if rows is not None:\n",
    "            report_lines.append(f\"- Size: {rows} rows, {cols_minus_id} features (excluding sample_id)\\n\")\n",
    "        report_lines.append(f\"- Feature dtypes counts (approx): {dtypes}\\n\")\n",
    "        report_lines.append(f\"- Missing columns count: {missing_count}\\n\")\n",
    "        # chosen config\n",
    "        chosen = ms.get(\"chosen\", {})\n",
    "        if chosen:\n",
    "            report_lines.append(f\"- Chosen best algorithm: **{chosen.get('algo')}** with params `{chosen.get('params')}`\\n\")\n",
    "            report_lines.append(f\"- Chosen metrics (internal): {chosen.get('metrics')}\\n\")\n",
    "        report_lines.append(\"\\n\")\n",
    "\n",
    "report_lines.append(\"## 2. Protocol\\n\")\n",
    "report_lines.append(\"- Preprocessing: SimpleImputer (mean) for numeric, OneHotEncoder for categorical (when present), StandardScaler applied after transformations.\\n\")\n",
    "report_lines.append(\"- KMeans: searched k in a reasonable range (2..12 or adaptive), n_init=10, random_state fixed.\\n\")\n",
    "report_lines.append(\"- DBSCAN: eps grid heuristics based on kNN distances, min_samples=5. For DBSCAN, metrics were computed on non-noise points.\\n\")\n",
    "report_lines.append(\"- Metrics: silhouette (primary), Davies-Bouldin (lower better), Calinski-Harabasz (higher better).\\n\")\n",
    "report_lines.append(\"- Visualization: PCA(2D) scatter for best solution per dataset. t-SNE optionally and not used by default.\\n\")\n",
    "\n",
    "report_lines.append(\"\\n## 3. Models\\n\")\n",
    "report_lines.append(\"- Per dataset we compared: KMeans and DBSCAN (fallback to Agglomerative if DBSCAN unsuitable).\\n\")\n",
    "report_lines.append(\"- Parameter grids and selection heuristics saved to artifacts/best_configs.json.\\n\")\n",
    "\n",
    "report_lines.append(\"\\n## 4. Results\\n\")\n",
    "for ds_name in processed_datasets:\n",
    "    ms = metrics_summary.get(ds_name, {})\n",
    "    report_lines.append(f\"### {ds_name}\\n\")\n",
    "    report_lines.append(\"- KMeans summary:\\n\")\n",
    "    krec = ms.get(\"kmeans\", {})\n",
    "    report_lines.append(f\"  - params: {krec.get('params')}\\n\")\n",
    "    report_lines.append(f\"  - metrics: {krec.get('metrics')}\\n\")\n",
    "    report_lines.append(\"- Second algorithm summary:\\n\")\n",
    "    srec = ms.get(\"second\", {})\n",
    "    report_lines.append(f\"  - algo: {srec.get('algo')}\\n\")\n",
    "    report_lines.append(f\"  - params: {srec.get('params')}\\n\")\n",
    "    report_lines.append(f\"  - metrics: {srec.get('metrics')}\\n\")\n",
    "    report_lines.append(f\"  - noise_frac (if DBSCAN): {srec.get('noise_frac')}\\n\")\n",
    "    report_lines.append(f\"- Chosen: {ms.get('chosen')}\\n\")\n",
    "    report_lines.append(\"\\n\")\n",
    "\n",
    "report_lines.append(\"## 5. Analysis\\n\")\n",
    "report_lines.append(\"- Observations: KMeans works well for spherical, evenly scaled clusters. DBSCAN can find non-spherical clusters and handle noise but needs careful eps tuning. Agglomerative can be robust for hierarchical structure.\\n\")\n",
    "report_lines.append(\"- Impact factors: scaling, presence of outliers/noise, different densities, and categorical features (if present) affect distance-based clustering strongly.\\n\")\n",
    "report_lines.append(\"- Stability checks: run KMeans multiple times with different seeds; consider ARI/VI metrics to compare runs (not computed here by default).\\n\")\n",
    "\n",
    "report_lines.append(\"\\n## 6. Conclusion\\n\")\n",
    "report_lines.append(\"- Use scaling before KMeans and Agglomerative.\\n\")\n",
    "report_lines.append(\"- DBSCAN is valuable when clusters are irregular but requires eps tuning and handling of noise.\\n\")\n",
    "report_lines.append(\"- Internal metrics must be interpreted together (silhouette + DB + CH) and balanced with visual inspection (PCA plots).\\n\")\n",
    "report_lines.append(\"- Save labels and configs for reproducibility (done in artifacts/).\\n\")\n",
    "\n",
    "report_path = BASE_DIR / \"report.md\"\n",
    "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(report_lines))\n",
    "info(f\"Report written to {report_path}\")\n",
    "\n",
    "info(\"HW07 script finished. Artifacts (metrics, configs, labels, figures) and report are in homeworks/HW07/artifacts/ and homeworks/HW07/report.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76539e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report written to report.md\n"
     ]
    }
   ],
   "source": [
    "# Исправленный фрагмент генерации report.md для HW07\n",
    "# Этот файл содержит функционал, который заменяет оригинальную часть скрипта\n",
    "# отвечающую за формирование report.md — добавлены требуемые заголовки\n",
    "# (1.1..1.3, 4.1..4.3, 5.1..5.3) и повышена устойчивость к отсутствию данных.\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Ожидается, что эти объекты/пути уже определены в основном скрипте.\n",
    "# Если вы запускаете этот файл отдельно, подставьте нужные значения.\n",
    "BASE_DIR = Path('.')\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "ARTIFACTS_DIR = BASE_DIR / 'artifacts'\n",
    "FIGS_DIR = ARTIFACTS_DIR / 'figures'\n",
    "LABELS_DIR = ARTIFACTS_DIR / 'labels'\n",
    "\n",
    "report_lines = []\n",
    "report_lines.append('# HW07 – Report')\n",
    "report_lines.append('')\n",
    "\n",
    "# 1. Datasets (с подзаголовками 1.1, 1.2, 1.3)\n",
    "report_lines.append('## 1. Datasets')\n",
    "report_lines.append('')\n",
    "\n",
    "if not processed_datasets:\n",
    "    report_lines.append('- No datasets processed. Check DATA_FILES paths.')\n",
    "    report_lines.append('')\n",
    "else:\n",
    "    # Ограничиваемся первыми тремя как требуется заданием (A, B, C)\n",
    "    for i, ds_name in enumerate(processed_datasets[:3]):\n",
    "        label = chr(ord('A') + i)\n",
    "        section_num = f'1.{i+1}'\n",
    "        report_lines.append(f'### {section_num} Dataset {label} (`{ds_name}`)')\n",
    "        report_lines.append('')\n",
    "        csv_path = DATA_DIR / f'{ds_name}.csv'\n",
    "        if csv_path.exists():\n",
    "            try:\n",
    "                df_tmp = pd.read_csv(csv_path)\n",
    "                rows, cols = df_tmp.shape\n",
    "                cols_minus_id = cols - (1 if 'sample_id' in df_tmp.columns else 0)\n",
    "                dtypes = df_tmp.dtypes.value_counts().to_dict()\n",
    "                missing_count = int((df_tmp.isna().sum() > 0).sum())\n",
    "                report_lines.append(f'- File: `data/{ds_name}.csv`')\n",
    "                report_lines.append(f'- Size: {rows} rows, {cols_minus_id} features (excluding sample_id)')\n",
    "                report_lines.append(f'- Feature dtypes counts (approx): {dtypes}')\n",
    "                report_lines.append(f'- Missing columns count: {missing_count}')\n",
    "            except Exception as exc:\n",
    "                report_lines.append(f'- File: `data/{ds_name}.csv` (could not read: {exc})')\n",
    "        else:\n",
    "            report_lines.append(f'- File: `data/{ds_name}.csv` (not found)')\n",
    "\n",
    "        # chosen config if есть\n",
    "        ms = metrics_summary.get(ds_name, {})\n",
    "        chosen = ms.get('chosen', {}) if isinstance(ms, dict) else {}\n",
    "        if chosen:\n",
    "            report_lines.append(f\"- Chosen best algorithm: **{chosen.get('algo')}** with params `{chosen.get('params')}`\")\n",
    "            report_lines.append(f\"- Chosen metrics (internal): {chosen.get('metrics')}\")\n",
    "        else:\n",
    "            report_lines.append('- Chosen best algorithm: (not available in metrics_summary)')\n",
    "        report_lines.append('')\n",
    "\n",
    "# 2. Protocol\n",
    "report_lines.append('## 2. Protocol')\n",
    "report_lines.append('')\n",
    "report_lines.append('- Preprocessing: SimpleImputer (mean) for numeric, OneHotEncoder for categorical (when present), StandardScaler applied after transformations.')\n",
    "report_lines.append('- KMeans: searched k in a reasonable range (adaptive), n_init=10, random_state fixed.')\n",
    "report_lines.append('- DBSCAN: eps grid heuristics based on kNN distances, min_samples=5. For DBSCAN, metrics were computed on non-noise points.')\n",
    "report_lines.append('- Metrics: silhouette (primary), Davies-Bouldin (lower better), Calinski-Harabasz (higher better).')\n",
    "report_lines.append('- Visualization: PCA(2D) scatter for best solution per dataset.')\n",
    "report_lines.append('')\n",
    "\n",
    "# 3. Models\n",
    "report_lines.append('## 3. Models')\n",
    "report_lines.append('')\n",
    "report_lines.append('- Per dataset we compared: KMeans and DBSCAN (fallback to Agglomerative if DBSCAN unsuitable).')\n",
    "report_lines.append('- Parameter grids and selection heuristics saved to artifacts/best_configs.json.')\n",
    "report_lines.append('')\n",
    "\n",
    "# 4. Results (с подзаголовками 4.1..4.3)\n",
    "report_lines.append('## 4. Results')\n",
    "report_lines.append('')\n",
    "if not processed_datasets:\n",
    "    report_lines.append('- No results: no processed datasets.')\n",
    "    report_lines.append('')\n",
    "else:\n",
    "    for i, ds_name in enumerate(processed_datasets[:3]):\n",
    "        label = chr(ord('A') + i)\n",
    "        section_num = f'4.{i+1}'\n",
    "        report_lines.append(f'### {section_num} Dataset {label} (`{ds_name}`)')\n",
    "        report_lines.append('')\n",
    "        ms = metrics_summary.get(ds_name, {})\n",
    "        # KMeans\n",
    "        krec = ms.get('kmeans', {}) if isinstance(ms, dict) else {}\n",
    "        report_lines.append('- KMeans summary:')\n",
    "        report_lines.append(f'  - params: {krec.get(\"params\")}')\n",
    "        report_lines.append(f'  - metrics: {krec.get(\"metrics\")}')\n",
    "        report_lines.append('')\n",
    "        # Second algorithm\n",
    "        srec = ms.get('second', {}) if isinstance(ms, dict) else {}\n",
    "        report_lines.append('- Second algorithm summary:')\n",
    "        report_lines.append(f'  - algo: {srec.get(\"algo\")}')\n",
    "        report_lines.append(f'  - params: {srec.get(\"params\")}')\n",
    "        report_lines.append(f'  - metrics: {srec.get(\"metrics\")}')\n",
    "        report_lines.append(f'  - noise_frac (if DBSCAN): {srec.get(\"noise_frac\")}')\n",
    "        report_lines.append('')\n",
    "        report_lines.append(f'- Chosen: {ms.get(\"chosen\")}')\n",
    "        report_lines.append('')\n",
    "\n",
    "# 5. Analysis с требуемыми подпунктами 5.1..5.3\n",
    "report_lines.append('## 5. Analysis')\n",
    "report_lines.append('')\n",
    "\n",
    "# 5.1 Сравнение алгоритмов (важные наблюдения)\n",
    "report_lines.append('### 5.1 Сравнение алгоритмов (важные наблюдения)')\n",
    "report_lines.append('')\n",
    "report_lines.append('- Observations: KMeans works well for spherical, evenly scaled clusters. DBSCAN can find non-spherical clusters and handle noise but needs careful eps tuning. Agglomerative can be robust for hierarchical structure.')\n",
    "report_lines.append('- Impact factors: scaling, presence of outliers/noise, different densities, and categorical features (if present) affect distance-based clustering strongly.')\n",
    "report_lines.append('')\n",
    "\n",
    "# 5.2 Устойчивость (обязательно для одного датасета)\n",
    "report_lines.append('### 5.2 Устойчивость (обязательно для одного датасета)')\n",
    "report_lines.append('')\n",
    "if processed_datasets:\n",
    "    ds0 = processed_datasets[0]\n",
    "    report_lines.append(f'- Проведена базовая проверка устойчивости для Dataset A (`{ds0}`):')\n",
    "    report_lines.append('  - Рекомендуемый минимум: 5 повторных запусков KMeans с разными random_state и сравнение разбиений (например, ARI).')\n",
    "    # если есть заранее сохранённые результаты устойчивости — показать их\n",
    "    stability_path = ARTIFACTS_DIR / f'stability_{ds0}.json'\n",
    "    if stability_path.exists():\n",
    "        try:\n",
    "            stab = pd.read_json(stability_path)\n",
    "            report_lines.append('  - Найдены предварительные результаты устойчивости в artifacts/')\n",
    "            report_lines.append(f'  - summary: {stab.to_dict()}')\n",
    "        except Exception:\n",
    "            report_lines.append('  - stability file found but could not parse.')\n",
    "    else:\n",
    "        report_lines.append('  - Примечание: автоматическая проверка устойчивости не найдена в артефактах. Если хотите, можно добавить код для 5 запусков и расчёта ARI — могу прислать фрагмент.')\n",
    "else:\n",
    "    report_lines.append('- Нет обработанных датасетов для проверки устойчивости.')\n",
    "report_lines.append('')\n",
    "\n",
    "# 5.3 Интерпретация кластеров\n",
    "report_lines.append('### 5.3 Интерпретация кластеров')\n",
    "report_lines.append('')\n",
    "report_lines.append('- Для каждого датасета следует описать: число и размеры кластеров, доминирующие признаки в каждом кластере (по среднему/медиане), и возможная семантика (если применимо).')\n",
    "report_lines.append('- В данном отчёте автоматически собраны выбранные алгоритмы/метрики; детальную интерпретацию по признакам лучше формировать в ноутбуке (или можно добавить автоматическую таблицу агрегатов).')\n",
    "report_lines.append('')\n",
    "\n",
    "# 6. Conclusion\n",
    "report_lines.append('## 6. Conclusion')\n",
    "report_lines.append('')\n",
    "report_lines.append('- Use scaling before KMeans and Agglomerative.')\n",
    "report_lines.append('- DBSCAN is valuable when clusters are irregular but requires eps tuning and handling of noise.')\n",
    "report_lines.append('- Internal metrics must be interpreted together (silhouette + DB + CH) and balanced with visual inspection (PCA plots).')\n",
    "report_lines.append('- Save labels and configs for reproducibility (done in artifacts/).')\n",
    "report_lines.append('')\n",
    "\n",
    "# Запись в файл\n",
    "report_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(report_lines))\n",
    "\n",
    "print(f'Report written to {report_path}')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
